{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X1ugnG39vvCD"
      },
      "outputs": [],
      "source": [
        "!pip install transformers\n",
        "import pandas as pd\n",
        "import torch\n",
        "import io\n",
        "import torch.nn.functional as F\n",
        "import random\n",
        "import numpy as np\n",
        "import time\n",
        "import math\n",
        "import datetime\n",
        "import torch.nn as nn\n",
        "from transformers import AutoModel, AutoTokenizer, AutoConfig\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "##Set random values\n",
        "seed_val = 40\n",
        "random.seed(seed_val)\n",
        "np.random.seed(seed_val)\n",
        "torch.manual_seed(seed_val)\n",
        "if torch.cuda.is_available():\n",
        "  torch.cuda.manual_seed_all(seed_val)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "\n",
        "# Import the AutoModelWithLMHead class\n",
        "from transformers import AutoModelWithLMHead\n"
      ],
      "metadata": {
        "id": "4ldWSiMly2lA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = AutoModelWithLMHead.from_pretrained('gpt2')\n",
        "!pip install sacremoses\n"
      ],
      "metadata": {
        "id": "Xmkd1ZA-y36y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "path=\"\"    #specify dataset path\n",
        "data=pd.read_csv(path)\n",
        "data.to_csv('data.csv', index=False)\n",
        "data.head()\n",
        "\n"
      ],
      "metadata": {
        "id": "SrJzTSt0y4MQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Separate the dataset into two based on the \"generated\" column value\n",
        "generated_data_new = data[data['generated'] == 1]\n",
        "human_data_new = data[data['generated'] == 0]\n",
        "\n",
        "human_data_new_train=human_data_new.sample(n=10000).reset_index(drop=True)\n",
        "\n",
        "human_data_new_test = human_data_new.iloc[1000:1010, :].reset_index(drop=True)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Example of how to save these datasets to separate files\n",
        "generated_data_new.to_csv('generated_data.csv', index=False)\n",
        "human_data_new.to_csv('human_data.csv', index=False)\n",
        "# print(generated_data_new.shape)\n",
        "print(human_data_new_train.shape)\n",
        "# print(generated_data_new.head())\n",
        "print(human_data_new_train.head())\n"
      ],
      "metadata": {
        "id": "FMhR2ja9zNzE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_new=data.sample(n=18000)\n",
        "data_new.to_csv('data_new.csv',index=False)"
      ],
      "metadata": {
        "id": "3b7uvFohzN7q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# If there's a GPU available...\n",
        "if torch.cuda.is_available():\n",
        "    # Tell PyTorch to use the GPU.\n",
        "    device = torch.device(\"cuda\")\n",
        "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
        "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
        "else :\n",
        "  device='cpu'"
      ],
      "metadata": {
        "id": "oY2mZJKrwAvo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#--------------------------------\n",
        "#  Transformer parameters\n",
        "#--------------------------------\n",
        "max_seq_length = 9160   #length of a sequential sentence\n",
        "batch_size = 32     #Determines the number of samples hat will be passed through to the network at one time.\n",
        "\n",
        "#--------------------------------\n",
        "#  GAN-BERT specific parameters\n",
        "#--------------------------------\n",
        "# number of hidden layers in the generator,\n",
        "# each of the size of the output space\n",
        "num_hidden_layers_g = 1;\n",
        "# number of hidden layers in the discriminator,\n",
        "# each of the size of the input space\n",
        "num_hidden_layers_d = 1;\n",
        "# size of the generator's input noisy vectors\n",
        "noise_size = 100\n",
        "# dropout to be applied to discriminator's input vectors\n",
        "out_dropout_rate = 0.2     #randomly selected neurons are ignored during training\n",
        "\n",
        "# Replicate labeled data to balance poorly represented datasets,\n",
        "# e.g., less than 1% of labeled material\n",
        "apply_balance = True\n",
        "\n",
        "#--------------------------------\n",
        "#  Optimization parameters\n",
        "#--------------------------------\n",
        "learning_rate_discriminator = 5e-5\n",
        "learning_rate_generator = 5e-5\n",
        "epsilon = 1e-8      #shows the change in o/p when a single sample is executed/\n",
        "num_train_epochs = 10\n",
        "multi_gpu = True             #It is supposed to run in single gpu\n",
        "# Scheduler to run the tasks at a specific time.\n",
        "apply_scheduler = False\n",
        "warmup_proportion = 0.1       #Its used to indicate set of training steps with very low learning rate.\n",
        "# Print\n",
        "print_each_n_step = 10\n",
        "\n",
        "#--------------------------------\n",
        "#  Adopted Tranformer model\n",
        "#--------------------------------\n",
        "\n",
        "model_name = \"bert-base-cased\"\n",
        "#model_name = \"bert-base-uncased\"\n",
        "#model_name = \"roberta-base\"\n",
        "#model_name = \"albert-base-v2\"\n",
        "#model_name = \"xlm-roberta-base\"\n",
        "#model_name = \"amazon/bort\"\n",
        "\n",
        "#--------------------------------\n",
        "#  Retrieve the TREC QC Dataset\n",
        "#--------------------------------\n",
        "! git clone https://github.com/crux82/ganbert"
      ],
      "metadata": {
        "id": "G1bfyXVqwDf1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_for_ganbert = human_data_new_train.iloc[0:1000,:] # use 1% of the labeled data for training\n",
        "\n",
        "data_unlabeled= human_data_new_train.drop(data_for_ganbert.index)\n",
        "#df_unlabeled=df_unlabeled.sample(frac=0.20)\n",
        "\n",
        "#df_unlabeled = df_unlabeled.drop(df_unlabeled.index)\n",
        "\n",
        "print(data_for_ganbert)\n",
        "data_for_ganbert.shape\n"
      ],
      "metadata": {
        "id": "5I4DXlIGwGAt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in data_unlabeled.index :\n",
        "  data_unlabeled['generated']=\"UNK\"\n",
        "label_list = ['UNK',0]"
      ],
      "metadata": {
        "id": "npxjIls1wGLn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"bert-base-cased\"\n",
        "transformer = AutoModel.from_pretrained(model_name)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)"
      ],
      "metadata": {
        "id": "pPsWYy8fwGN-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def generate_data_loader(input_examples, label_masks, label_map, do_shuffle=False, balance_label_examples=False):\n",
        "    examples = []\n",
        "\n",
        "    if len(input_examples) != len(label_masks):\n",
        "        raise ValueError(\"Lengths of input_examples and label_masks do not match.\")\n",
        "\n",
        "    num_labeled_examples = sum(label_masks)\n",
        "    label_mask_rate = num_labeled_examples / len(input_examples)\n",
        "\n",
        "    for index, ex in input_examples.iterrows():\n",
        "        if index >= len(label_masks):\n",
        "            raise IndexError(f\"Index {index} is out of bounds for axis 0 with size {len(label_masks)}\")\n",
        "        if label_mask_rate == 1 or not balance_label_examples:\n",
        "            examples.append((ex, label_masks[index]))\n",
        "        else:\n",
        "            if label_masks[index]:\n",
        "                balance = int(1 / label_mask_rate)\n",
        "                balance = int(math.log(balance, 2))\n",
        "                if balance < 1:\n",
        "                    balance = 1\n",
        "                for b in range(0, int(balance)):\n",
        "                    examples.append((ex, label_masks[index]))\n",
        "            else:\n",
        "                examples.append((ex, label_masks[index]))\n",
        "\n",
        "    input_ids = []\n",
        "    input_mask_array = []\n",
        "    label_mask_array = []\n",
        "    label_id_array = []\n",
        "\n",
        "    for (text, label_mask) in examples:\n",
        "        encoded_sent = tokenizer.encode(text[0], add_special_tokens=True, max_length=max_seq_length, padding=\"max_length\", truncation=True)\n",
        "        input_ids.append(encoded_sent)\n",
        "        label_id_array.append(label_map[text[1]])\n",
        "        label_mask_array.append(label_mask)\n",
        "\n",
        "    for sent in input_ids:\n",
        "        att_mask = [int(token_id > 0) for token_id in sent]\n",
        "        input_mask_array.append(att_mask)\n",
        "\n",
        "    input_ids = torch.tensor(input_ids)\n",
        "    input_mask_array = torch.tensor(input_mask_array)\n",
        "    label_id_array = torch.tensor(label_id_array, dtype=torch.long)\n",
        "    label_mask_array = torch.tensor(label_mask_array)\n",
        "\n",
        "    dataset = TensorDataset(input_ids, input_mask_array, label_id_array, label_mask_array)\n",
        "\n",
        "    if do_shuffle:\n",
        "        sampler = RandomSampler(dataset)\n",
        "    else:\n",
        "        sampler = SequentialSampler(dataset)\n",
        "\n",
        "    return DataLoader(dataset, sampler=sampler, batch_size=32)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "haClZYx_wGQQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "label_map = {}\n",
        "for (i, label) in enumerate(label_list):\n",
        "  label_map[label] = i\n",
        "#------------------------------\n",
        "#   Load the train dataset\n",
        "#------------------------------\n",
        "train_examples = data_for_ganbert.copy()\n",
        "#The labeled (train) dataset is assigned with a mask set to True\n",
        "train_label_masks = np.ones(len( data_for_ganbert), dtype=bool)\n",
        "#If unlabel examples ar available\n",
        "\n",
        "train_examples =pd.concat([train_examples,data_unlabeled],axis=0).reset_index(drop=True)\n",
        "  #The unlabeled (train) dataset is assigned with a mask set to False\n",
        "tmp_masks = np.zeros(len(data_unlabeled), dtype=bool)\n",
        "train_label_masks = np.concatenate([train_label_masks,tmp_masks])\n",
        "\n",
        "train_dataloader = generate_data_loader(train_examples, train_label_masks, label_map, do_shuffle = False, balance_label_examples = apply_balance)\n",
        "#   Load the test dataset\n",
        "#------------------------------\n",
        "#The labeled (test) dataset is assigned with a mask set to True\n",
        "test_label_masks = np.ones(len(human_data_new_test), dtype=bool)\n",
        "\n",
        "test_dataloader = generate_data_loader(human_data_new_test, test_label_masks, label_map, do_shuffle = False, balance_label_examples = False)\n",
        "#------------------------------\n"
      ],
      "metadata": {
        "id": "eNCMp_uSwGTp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for batch in train_dataloader:\n",
        "    input_ids, input_mask, labels, label_mask = batch\n",
        "    print(\"Input IDs shape:\", input_ids.shape)\n",
        "    print(\"Input Mask shape:\", input_mask.shape)\n",
        "    print(\"Labels shape:\", labels.shape)\n",
        "    print(\"Label Mask shape:\", label_mask.shape)\n",
        "    break  # Print only the shape of the first batch\n"
      ],
      "metadata": {
        "id": "mS9Se4tQwcob"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#------------------------------\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, input_size=768, hidden_sizes=[768], num_labels=2, dropout_rate=0.1):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.input_dropout = nn.Dropout(p=dropout_rate)\n",
        "        layers = []\n",
        "        hidden_sizes = [input_size] + hidden_sizes\n",
        "        for i in range(len(hidden_sizes)-1):\n",
        "            layers.extend([nn.Linear(hidden_sizes[i], hidden_sizes[i+1]), nn.LeakyReLU(0.2, inplace=True), nn.Dropout(dropout_rate)])\n",
        "\n",
        "        self.layers = nn.Sequential(*layers) #per il flatten\n",
        "        self.logit = nn.Linear(hidden_sizes[-1],num_labels)\n",
        "        #nn linear is a module which is used to create a single layer feed-forward network\n",
        "        self.softmax = nn.Softmax(dim=-1)\n",
        "\n",
        "    def forward(self, input_rep):\n",
        "        input_rep = self.input_dropout(input_rep)\n",
        "        last_rep = self.layers(input_rep)\n",
        "        #Logits simply means that the function operates on the unscaled output of earlier layers and that the relative scale to understand the units is linear. It means, in particular, the sum of the inputs may not equal 1\n",
        "        logits = self.logit(last_rep)\n",
        "        probs = self.softmax(logits)\n",
        "        return last_rep, logits, probs"
      ],
      "metadata": {
        "id": "c8O7Zidhwg8c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"bert-base-cased\""
      ],
      "metadata": {
        "id": "vN9t40Cxwcyi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The config file is required to get the dimension of the vector produced by\n",
        "# the underlying transformer\n",
        "config = AutoConfig.from_pretrained(model_name)\n",
        "hidden_size = int(config.hidden_size)\n",
        "# Define the number and width of hidden layers\n",
        "hidden_levels_g = [hidden_size for i in range(0, num_hidden_layers_g)]\n",
        "hidden_levels_d = [hidden_size for i in range(0, num_hidden_layers_d)]\n",
        "\n",
        "#-------------------------------------------------\n",
        "#   Instantiate the Generator and Discriminator\n",
        "#-------------------------------------------------\n",
        "# generator = Generator(noise_size=noise_size, output_size=hidden_size, hidden_sizes=hidden_levels_g, dropout_rate=out_dropout_rate)\n",
        "discriminator = Discriminator(input_size=hidden_size, hidden_sizes=hidden_levels_d, num_labels=len(label_list), dropout_rate=out_dropout_rate)\n",
        "\n",
        "# Put everything in the GPU if available\n",
        "if torch.cuda.is_available():\n",
        "  # generator.cuda()\n",
        "  discriminator.cuda()\n",
        "  transformer.cuda()\n",
        "  if multi_gpu:\n",
        "    transformer = torch.nn.DataParallel(transformer)\n",
        "\n",
        "# print(config)"
      ],
      "metadata": {
        "id": "im2G_TB0woH0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers\n",
        "import pandas as pd\n",
        "import torch\n",
        "import io\n",
        "import torch.nn.functional as F\n",
        "import random\n",
        "import numpy as np\n",
        "import time\n",
        "import math\n",
        "import datetime\n",
        "import torch.nn as nn\n",
        "from transformers import AutoModel, AutoTokenizer, AutoConfig\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "##Set random values\n",
        "seed_val = 40\n",
        "random.seed(seed_val)\n",
        "np.random.seed(seed_val)\n",
        "torch.manual_seed(seed_val)\n",
        "if torch.cuda.is_available():\n",
        "  torch.cuda.manual_seed_all(seed_val)\n",
        "model_name = \"bert-base-cased\"\n",
        "transformer = AutoModel.from_pretrained(model_name)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "input_ids=[]\n",
        "max_seq_length=100\n",
        "for text in data_new['text']:\n",
        "  encoded_sent = tokenizer.encode(text, add_special_tokens=True, max_length=max_seq_length, padding=\"max_length\", truncation=True)\n",
        "  input_ids.append(encoded_sent)\n",
        "\n"
      ],
      "metadata": {
        "id": "4-8RSXoVwoO-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_ids = torch.tensor(input_ids)"
      ],
      "metadata": {
        "id": "bLn5sZEpwoaE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataloader1 = DataLoader(input_ids, batch_size=32, shuffle=False)"
      ],
      "metadata": {
        "id": "NMdOb7nzw0jw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "# Check if GPU is available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Learning rate\n",
        "lr = 1e-4\n",
        "batch_size=32\n",
        "\n",
        "# Move models to the GPU\n",
        "transformer.to(device)\n",
        "discriminator.to(device)\n",
        "\n",
        "# Optimizer for the discriminator\n",
        "optimizer_d = optim.Adam(discriminator.parameters(), lr=lr)\n",
        "\n",
        "# Number of epochs\n",
        "num_epochs = 8\n",
        "\n",
        "for epoch_i in range(num_epochs):\n",
        "    loss1 = 0\n",
        "    i = 0  # Reset the index for each epoch\n",
        "\n",
        "    # Set the discriminator to training mode\n",
        "    discriminator.train()\n",
        "\n",
        "    for batch in dataloader1:\n",
        "        # If batch is a list of tensors, stack them into a single tensor\n",
        "        if isinstance(batch, list):\n",
        "            batch = torch.stack(batch).to(device)\n",
        "        else:\n",
        "            batch = batch.to(device)\n",
        "\n",
        "        batch_size = batch.size(0)\n",
        "\n",
        "        # Model outputs\n",
        "        model_outputs = transformer(batch)\n",
        "        hidden_states = model_outputs.last_hidden_state[:, 0, :]\n",
        "        features, logits, probs = discriminator(hidden_states)\n",
        "\n",
        "        # Calculate log probabilities\n",
        "        log_probs = F.log_softmax(logits, dim=-1)\n",
        "\n",
        "        # Convert the labels to a PyTorch tensor before one-hot encoding and move to GPU\n",
        "        labels = torch.tensor(data_new.iloc[i:i+batch_size, -1].values, dtype=torch.long).to(device)\n",
        "        label2one_hot = F.one_hot(labels, num_classes=2).float().to(device)\n",
        "\n",
        "        # Calculate per example loss\n",
        "        per_example_loss = -torch.sum(label2one_hot * log_probs, dim=-1)\n",
        "\n",
        "        ans = torch.mean(per_example_loss)\n",
        "        loss1 += ans.item()\n",
        "\n",
        "        # Discriminator loss\n",
        "        d_loss = ans\n",
        "\n",
        "        # Zero the parameter gradients\n",
        "        optimizer_d.zero_grad()\n",
        "\n",
        "        # Backward pass\n",
        "        d_loss.backward()\n",
        "\n",
        "        # Optimization step\n",
        "        optimizer_d.step()\n",
        "\n",
        "        # Update the index\n",
        "        i += batch_size\n",
        "\n",
        "    # Calculate the average loss\n",
        "    avg_loss = loss1 / len(dataloader1)\n",
        "    print(f\"Epoch {epoch_i + 1}: avg_loss: {avg_loss}\")\n",
        "    # Save checkpoint after each epoch\n",
        "    torch.save({\n",
        "        'epoch': epoch_i,\n",
        "        'model_state_dict': discriminator.state_dict(),\n",
        "        'optimizer_state_dict': optimizer_d.state_dict(),\n",
        "    }, '.pth') # specify path where you want to store your supervised trained model\n",
        "\n"
      ],
      "metadata": {
        "id": "VEVvR16sw0pz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_ids_test=[]\n",
        "data_new_test=data.sample(n=1000)\n",
        "for text in data_new_test['text']:\n",
        "  encoded_sent = tokenizer.encode(text, add_special_tokens=True, max_length=50, padding=\"max_length\", truncation=True)\n",
        "  input_ids_test.append(encoded_sent)\n",
        "input_ids_test = torch.tensor(input_ids_test)\n",
        "test_dataloader= DataLoader(input_ids_test, batch_size=4, shuffle=False)"
      ],
      "metadata": {
        "id": "OcBJ8kXPw0tX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "# Set the discriminator to evaluation mode\n",
        "discriminator.eval()\n",
        "transformer.eval()\n",
        "\n",
        "# List to store the true labels and predicted labels\n",
        "true_labels = []\n",
        "predicted_labels = []\n",
        "i=0\n",
        "with torch.no_grad():\n",
        "    for batch in test_dataloader:\n",
        "        # If batch is a list of tensors, stack them into a single tensor\n",
        "        if isinstance(batch, list):\n",
        "            batch = torch.stack(batch).to(device)\n",
        "        else:\n",
        "            batch = batch.to(device)\n",
        "\n",
        "        batch_size = batch.size(0)\n",
        "\n",
        "        # Model outputs\n",
        "        model_outputs = transformer(batch)\n",
        "        hidden_states = model_outputs.last_hidden_state[:,0,:]\n",
        "        features, logits, probs = discriminator(hidden_states)\n",
        "\n",
        "        # Get the predicted labels\n",
        "        _, preds = torch.max(logits, dim=1)\n",
        "\n",
        "        # Convert the labels to a PyTorch tensor and move to GPU\n",
        "        labels = torch.tensor(data_new_test.iloc[i:i+batch_size, -1].values, dtype=torch.long).to(device)\n",
        "\n",
        "        # Append the true labels and predictions to the lists\n",
        "        true_labels.extend(labels.cpu().numpy())\n",
        "        predicted_labels.extend(preds.cpu().numpy())\n",
        "        i+=batch_size\n",
        "\n",
        "# Calculate metrics\n",
        "accuracy = accuracy_score(true_labels, predicted_labels)\n",
        "precision = precision_score(true_labels, predicted_labels, average='weighted')\n",
        "recall = recall_score(true_labels, predicted_labels, average='weighted')\n",
        "f1 = f1_score(true_labels, predicted_labels, average='weighted')\n",
        "\n",
        "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
        "print(f\"Test Precision: {precision:.4f}\")\n",
        "print(f\"Test Recall: {recall:.4f}\")\n",
        "print(f\"Test F1 Score: {f1:.4f}\")"
      ],
      "metadata": {
        "id": "rNM34yFYx-Ch"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer, BertModel, BertTokenizer\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# Load checkpoint for discriminator\n",
        "checkpoint = torch.load('.pth') #specify the path where you have stored your supervised trained model.\n",
        "start_epoch = checkpoint['epoch'] + 1  # Continue from the next epoch\n",
        "\n",
        "# Load GPT-2 tokenizer and model\n",
        "gpt2_tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "gpt2_model = GPT2LMHeadModel.from_pretrained('gpt2',pad_token_id=tokenizer.eos_token_id).to(device)\n",
        "gpt2_tokenizer.pad_token=gpt2_tokenizer.eos_token\n",
        "# Add a padding token to GPT-2 tokenizer if it doesn't have one\n",
        "# if gpt2_tokenizer.pad_token is None:\n",
        "#     gpt2_tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
        "#     gpt2_model.resize_token_embeddings(len(gpt2_tokenizer))\n",
        "# Set pad_token_id to eos_token_id for open-end generation\n",
        "# gpt2_model.config.pad_token_id = gpt2_tokenizer.eos_token_id\n",
        "# Load BERT tokenizer and model\n",
        "bert_tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
        "bert_model = BertModel.from_pretrained('bert-base-cased').to(device)\n",
        "\n",
        "# Define a custom dataset class for the essays\n",
        "class EssayDataset(Dataset):\n",
        "    def __init__(self, essays, tokenizer, max_length=15):\n",
        "        self.essays = essays\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.essays)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        essay = self.essays[idx]\n",
        "        first_five_words = ' '.join(essay.split()[:15])\n",
        "        inputs = self.tokenizer(first_five_words, return_tensors='pt', max_length=self.max_length, truncation=True,padding='max_length')\n",
        "        inputs = {key: val.squeeze(0) for key, val in inputs.items()}\n",
        "        return inputs\n",
        "\n",
        "# Example essays dataset\n",
        "essays = human_data_new_train['text']\n",
        "\n",
        "# Create dataset and dataloader\n",
        "dataset = EssayDataset(essays, gpt2_tokenizer)\n",
        "dataloader = DataLoader(dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "# Setting up the device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "training_stats = []\n",
        "\n",
        "# Measure the total training time for the whole run.\n",
        "total_t0 = time.time()\n",
        "\n",
        "\n",
        "# # Models parameters\n",
        "# transformer_vars = [i for i in transformer.parameters()]\n",
        "# d_vars = transformer_vars + [v for v in discriminator.parameters()]\n",
        "# g_vars = [v for v in generator.parameters()]\n",
        "\n",
        "# Optimizer\n",
        "lr=1e-4\n",
        "optimizer_g = optim.AdamW(gpt2_model.parameters(), lr=lr)\n",
        "optimizer_d = optim.Adam(discriminator.parameters(), lr=lr)\n",
        "epsilon = 1e-8\n",
        "# Load state for the discriminator optimizer\n",
        "optimizer_d.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "discriminator.load_state_dict(checkpoint['model_state_dict'])\n",
        "discriminator.to(device)\n",
        "\n",
        "# Scheduler\n",
        "if apply_scheduler:\n",
        "    num_train_examples = len(train_examples)\n",
        "    num_train_steps = int(num_train_examples / batch_size * num_train_epochs)\n",
        "    num_warmup_steps = int(num_train_steps * warmup_proportion)\n",
        "\n",
        "    scheduler_d = get_constant_schedule_with_warmup(optimizer_d, num_warmup_steps=num_warmup_steps)\n",
        "    scheduler_g = get_constant_schedule_with_warmup(optimizer_g, num_warmup_steps=num_warmup_steps)\n",
        "\n",
        "# Define the maximum sequence length for the transformer model\n",
        "max_seq_length = 100\n",
        "\n",
        "# Function to truncate the input sequences\n",
        "def truncate_sequences(input_ids, input_mask, labels, label_mask, max_length):\n",
        "    return (input_ids[:, :max_length],\n",
        "            input_mask[:, :max_length],\n",
        "            labels,  # Assuming labels don't need truncation\n",
        "            label_mask)  # Assuming label_mask doesn't need truncation\n",
        "# Compute and cache real data embeddings\n",
        "real_data_embeddings = []\n",
        "\n",
        "for batch in train_dataloader:\n",
        "    b_input_ids = batch[0].to(device)\n",
        "    b_input_mask = batch[1].to(device)\n",
        "    b_labels = batch[2].to(device)\n",
        "    b_label_mask = batch[3].to(device)\n",
        "\n",
        "    # Truncate sequences\n",
        "    b_input_ids, b_input_mask, b_labels, b_label_mask = truncate_sequences(b_input_ids, b_input_mask, b_labels, b_label_mask, max_seq_length)\n",
        "\n",
        "    # Encode real data in the Transformer\n",
        "    with torch.no_grad():\n",
        "        model_outputs = transformer(b_input_ids, attention_mask=b_input_mask)\n",
        "        hidden_states = model_outputs.last_hidden_state[:, 0, :]\n",
        "        real_data_embeddings.append(hidden_states)\n",
        "\n",
        "# For each epoch...\n",
        "for epoch_i in range(start_epoch, 20):\n",
        "    # ========================================\n",
        "    #               Training\n",
        "    # ========================================\n",
        "    # Perform one full pass over the training set.\n",
        "    print(\"\")\n",
        "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, num_train_epochs))\n",
        "    print('Training...')\n",
        "\n",
        "    # Measure how long the training epoch takes.\n",
        "    t0 = time.time()\n",
        "\n",
        "    # Reset the total loss for this epoch.\n",
        "    tr_g_loss = 0\n",
        "    tr_d_loss = 0\n",
        "\n",
        "    # Put the model into training mode.\n",
        "    transformer.train()\n",
        "    # generator.train()\n",
        "    discriminator.train()\n",
        "    gpt2_model.train()\n",
        "\n",
        "    # For each batch of training data...\n",
        "    for (step, batch), gen_batch, real_embedding in zip(enumerate(train_dataloader), dataloader, real_data_embeddings):\n",
        "        # Unpack this training batch from our dataloader.\n",
        "        b_input_ids = batch[0].to(device)\n",
        "        b_input_mask = batch[1].to(device)\n",
        "        b_labels = batch[2].to(device)\n",
        "        b_label_mask = batch[3].to(device)\n",
        "\n",
        "        # Truncate sequences\n",
        "        b_input_ids, b_input_mask, b_labels, b_label_mask = truncate_sequences(b_input_ids, b_input_mask, b_labels, b_label_mask, max_seq_length)\n",
        "        real_batch_size = b_input_ids.shape[0]\n",
        "\n",
        "\n",
        "\n",
        "        # Generate fake data using GPT-2\n",
        "        input_ids = gen_batch['input_ids'].to(device)\n",
        "        attention_mask = gen_batch['attention_mask'].to(device)\n",
        "\n",
        "        # Generate text\n",
        "        seq_len = 100\n",
        "        generated_ids = gpt2_model.generate(input_ids,attention_mask=attention_mask, max_length=seq_len,do_sample=True, num_beams=5, no_repeat_ngram_size=2, early_stopping=True)\n",
        "        generated_text = [gpt2_tokenizer.decode(g, skip_special_tokens=True) for g in generated_ids]\n",
        "\n",
        "        # Fake data features from GPT-2\n",
        "        generated_inputs = bert_tokenizer(generated_text, return_tensors='pt', padding=True, truncation=True, max_length=seq_len).to(device)\n",
        "        gen_input_ids = generated_inputs['input_ids'].to(device)\n",
        "        gen_attention_mask = generated_inputs['attention_mask'].to(device)\n",
        "\n",
        "\n",
        "        # generated_outputs = gpt2_model(**generated_inputs)\n",
        "        # # gen_rep = generated_outputs[-1]\n",
        "        # gen_rep = generated_outputs.hidden_states[-1]\n",
        "        # Enable the return of hidden states for the GPT-2 model\n",
        "        # with torch.no_grad():\n",
        "        #     generated_outputs = gpt2_model(**generated_inputs, output_hidden_states=True)\n",
        "        # gen_rep = generated_outputs.hidden_states[-1][:, 0, :]\n",
        "        model_outputs_gen = transformer(gen_input_ids, attention_mask= gen_attention_mask)\n",
        "        gen_rep = model_outputs_gen.last_hidden_state[:,0,:]\n",
        "\n",
        "        # Generate the output of the Discriminator for real and fake data.\n",
        "        # First, we put together the output of the transformer and the generator\n",
        "        discriminator_input = torch.cat([real_embedding, gen_rep], dim=0)\n",
        "\n",
        "        # Then, we select the output of the discriminator\n",
        "        features, logits, probs = discriminator(discriminator_input)\n",
        "\n",
        "        # Finally, we separate the discriminator's output for the real and fake data\n",
        "        features_list = torch.split(features, real_batch_size)\n",
        "        D_real_features = features_list[0]\n",
        "        D_fake_features = features_list[1]\n",
        "\n",
        "        logits_list = torch.split(logits, real_batch_size)\n",
        "        D_real_logits = logits_list[0]\n",
        "        D_fake_logits = logits_list[1]\n",
        "\n",
        "        probs_list = torch.split(probs, real_batch_size)\n",
        "        D_real_probs = probs_list[0]\n",
        "        D_fake_probs = probs_list[1]\n",
        "\n",
        "        #---------------------------------\n",
        "        #  LOSS evaluation\n",
        "        #---------------------------------\n",
        "        # Generator's LOSS estimation\n",
        "        g_loss_d = -1 * torch.mean(torch.log(1 - D_fake_probs[:,-1] + epsilon))\n",
        "        g_feat_reg = torch.mean(torch.pow(torch.mean(D_real_features, dim=0) - torch.mean(D_fake_features, dim=0), 2))\n",
        "        g_loss = g_loss_d + g_feat_reg\n",
        "\n",
        "        # Discriminator's LOSS estimation\n",
        "        D_L_unsupervised1U = -1 * torch.mean(torch.log(1 - D_real_probs[:, -1] + epsilon))\n",
        "        D_L_unsupervised2U = -1 * torch.mean(torch.log(D_fake_probs[:, -1] + epsilon))\n",
        "        d_loss = D_L_unsupervised1U + D_L_unsupervised2U\n",
        "\n",
        "        #---------------------------------\n",
        "        #  OPTIMIZATION\n",
        "        #---------------------------------\n",
        "        # Avoid gradient accumulation\n",
        "        optimizer_g.zero_grad()\n",
        "        optimizer_d.zero_grad()\n",
        "\n",
        "        # Calculate weight updates\n",
        "        g_loss.backward(retain_graph=True)\n",
        "        d_loss.backward()\n",
        "\n",
        "        # Apply modifications\n",
        "        optimizer_g.step()\n",
        "        optimizer_d.step()\n",
        "\n",
        "        # Save the losses to print them later\n",
        "        tr_g_loss += g_loss.item()\n",
        "        tr_d_loss += d_loss.item()\n",
        "\n",
        "        # Update the learning rate with the scheduler\n",
        "        if apply_scheduler:\n",
        "            scheduler_d.step()\n",
        "            scheduler_g.step()\n",
        "\n",
        "    # Calculate the average loss over all of the batches.\n",
        "    avg_train_loss_g = tr_g_loss / len(train_dataloader)\n",
        "    avg_train_loss_d = tr_d_loss / len(train_dataloader)\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"  Average training loss generator: {0:.3f}\".format(avg_train_loss_g))\n",
        "    print(\"  Average training loss discriminator: {0:.3f}\".format(avg_train_loss_d))\n",
        "    # Save checkpoint after each epoch\n",
        "    torch.save({\n",
        "        'epoch': epoch_i,\n",
        "        'model_state_dict': discriminator.state_dict(),\n",
        "        'optimizer_state_dict': optimizer_d.state_dict(),\n",
        "    }, '.pth') # provide path where you want to store your final trained model.\n"
      ],
      "metadata": {
        "id": "f01oiRl1x-wR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EYeVOb8Fx-4k"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}